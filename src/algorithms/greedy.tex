\documentclass{standalone}
\begin{document}

\section{Greedy}

The greedy algorithm design paradigm produces straightforward and fast solutions to
certain problems. Usually, however, greedy algorithms do not produce correct
results, and great care must be taken to prove their correctness.

In general, the strategy is to choose a locally optimal solution in the hopes
that it produces a globally optimal output. Proofs of correctness and optimality
usually involve an exchange argument and/or induction.

\subsection{Dijkstra's Shortest Path Algorithm}

The canonical greedy algorithm. Dijkstra's algorithm computes the shortest paths
from a starting vertex by choose the least costly edge spanning a graph "cut"
or partition incident to the visited nodes. In other words, greedily choose the
edge which minimizes the current distance traveled.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}[
  vertex/.style={circle, draw, fill=white, thick, minimum size=20},
  ]

  % Vertices
  \node[vertex] (S) {$S$};
  \node[vertex] (V) [above right=of S] {$V$};
  \node[vertex] (W) [below right=of S] {$W$};
  \node[vertex] (T) [below right=of V] {$T$};

  % Edges
  \draw[->, thick] (S) to node[above] {1} (V);
  \draw[->, thick] (S) to node[below] {4} (W);
  \draw[->, thick] (V) to node[right] {2} (W);
  \draw[->, thick] (V) to node[above] {6} (T);
  \draw[->, thick] (W) to node[below] {3} (T);
  \end{tikzpicture}
  \caption{Weighted, directed graph}
  \label{fig:dijkstra1}
\end{figure}

The shortest path algorithm outputs the following when starting from $S$:

\begin{tabular}{ |c|c| }
  \hline
  Vertex & Shortest Path \\
  \hline
  S & 0 \\
  \hline
  V & 1 \\
  \hline
  W & 3 \\
  \hline
  T & 6 \\
  \hline
\end{tabular}

\begin{algorithm}
\caption{Dijkstra's Shortest-Path}
\begin{algorithmic}
  \Require $G = (V, E)$ has nonnegative edge weights
  \State $X = \{s\}$
  \State len(s) = 0, len(v) = $+\infty$ for every $v \neq s$ \\
  \While{There exists an edge $(v, w) \; v \in X, w \not \in X$ }
    \State $(v', w')$ = edge minimizing $len(v) + l_{vw}$
    \State Add $w'$ to $X$
    \State len(w') = len(v') + $l_{v'w'}$ \\
  \Return len
  \EndWhile
\end{algorithmic}
\end{algorithm}

The proof of Dijkstra's shortest-path algorithm differs from that of Prim's and
Kruskal's in that it proceeds by basic induction rather than presenting a proof
by contradiction and exchange argument\footnote{Consequently, I find this proof
to be slightly more difficult}.

First, a precise statement of the theorem:

\begin{theorem} \label{thm:dijkstra}
  For every directed graph $G = (V, E)$, for every starting vertex $s \in V$,
  Dijkstra's shortest-path algorithm outputs the distance of the shortest paths
  from $s$ to every (reachable) $v \in V$.
\end{theorem}

\begin{proof}[Proof of Dijkstra's Shortest-Path Algorithm] \label{prf:dijkstra}
  Some initial setup. Let $k$ be the number of a vertex added to the solution,
  and $P(k)$ be the statement that distance from the starting vertex $s$ to the
  $k$th vertex, say $v$, is the shortest such distance.

  During this proof, we'll denote the \emph{true} shortest-path distance with
  $dist(s, v), \; v \in V$ and the \emph{computed} value to be $len(v)$. Our
  job is to prove that these values are equal at every iteration.

  Proceeding by induction, consider the graph consisting of only $s$. In this
  case $n = 1$, and by the definition of the algorithm, $dist(s, s) = 0$. Now
  assume the correctness of Dijkstra's for all $N \geqslant n$ and consider a
  graph with $n + 1$ vertices. By the inductive hypothesis, all of $P(1) \dots
  P(n)$ hold, and so we consider the moment the algorithm must choose to add
  vertex $n + 1$ to the solution.

  Label the edge the algorithm chooses $(v', w')$. The distance from $s$ to $w'$
  is given by:

  \[
    len(w') = len(v') + weight((v', w'))
  \]

  By the inductive hypothesis, we know $len(v') = dist(s, v')$. What remains to
  be shown is that adding edge $(v', w')$ in the above formula is the optimal
  choice at the $n+1$st iteration.

  TODO: ?
\end{proof}

\subsubsection{Analysis}

The cost of repeatedly selecting the minimum edge via brute force dominates the runtime of the algorithm. For $G = (V, E)$, we traverse $|V|$ nodes, each time performing $O(|E|)$ work to select the minimum edge crossing the cut. Therefore, the naive version of Dijkstra's runs in $O(|E||V|)$.

Using a [[Heap]] this algorithm can be sped up significantly, achieving $O((|E| + |V|)\log|V|)$ runtime.

\subsection{Prim's MST Algorithm}

Prim's algorithm for computing a minimum-spanning tree. The algorithm functions almost identically to Dijkstra's algorithm in that the minimum incident edge is always chosen for the solution.

\begin{tikzpicture}[
vertex/.style={circle, draw, fill=white, thick, minimum size=20},
]

% Vertices
\node[vertex] (a) {$a$};
\node[vertex] (b) [right=of a] {$b$};
\node[vertex] (c) [below=of a] {$c$};
\node[vertex] (d) [below=of b] {$d$};

% Edges
\draw[-, thick] (a) to node[above] {1} (b);
\draw[-, thick] (a) to node[above] {3} (d);
\draw[-, thick] (a) to node[left] {4} (c);
\draw[-, thick] (b) to node[right] {2} (d);
\draw[-, thick] (c) to node[below] {5} (d);
\end{tikzpicture}

\begin{algorithm}
\caption{Prim's MST Algorithm}
\begin{algorithmic}
  \State $X = \{s\}$
  \State $T = \emptyset$
  \While{There is an edge $(v, w)$ s.t $v \in X$, $w \not \in X$}
    \State $(v', w')$ = minimum cost edge \\
    \State Add $w'$ to $X$
    \State Add $(v', w')$ to $T$
  \EndWhile
  \Return $T$
\end{algorithmic}
\end{algorithm}

\subsubsection{Analysis}

Once again the cost of repeatedly selecting the minimum edge dominates the
runtime of the algorithm leading to $O(|V||E|)$ worst-case runtime.

We can employ the same technique of using a Heap as in Dijkstra's algorithm to
achieve a runtime of $O(|E|\log|V|)$.

\subsection{Kruskal's MST Algorithm}

Kruskal's algorithm adopts a different approach, instead opting to choose the
minimum edge that would not introduce a cycle.

\begin{tikzpicture}[
vertex/.style={circle, draw, fill=white, thick, minimum size=20},
]

% Vertices
\node[vertex] (a) {$a$};
\node[vertex] (b) [right=of a] {$b$};
\node[vertex] (c) [below=of a] {$c$};
\node[vertex] (d) [below=of b] {$d$};

% Edges
\draw[-, thick] (a) to node[above] {1} (b);
\draw[-, thick] (a) to node[above] {3} (d);
\draw[-, thick] (a) to node[left] {4} (c);
\draw[-, thick] (b) to node[right] {2} (d);
\draw[-, thick] (c) to node[below] {5} (d);
\end{tikzpicture}

Kruskal's algorithm would execute the following steps:

\begin{enumerate}
  \item Choose $(a, b)$, since it is the minimum cost edge
  \item Choose $(b, d)$, since no cycle is produced
  \item Choose $(d, e)$, since $(a, c)$ would produce a cycle
  \item The chosen edges form a spanning tree of minimum cost!
\end{enumerate}

\begin{figure}[h]
  \centering
  \begin{tikzpicture}[
  vertex/.style={circle, draw, fill=white, thick, minimum size=20},
  ]

  % Vertices
  \node[vertex] (a) {$a$};
  \node[vertex] (b) [right=of a] {$b$};
  \node[vertex] (c) [below=of a] {$c$};
  \node[vertex] (d) [below=of b] {$d$};

  % Edges
  \draw[-, thick] (a) to node[above] {1} (b);
  \draw[-, thick] (a) to node[left] {4} (c);
  \draw[-, thick] (b) to node[right] {2} (d);
  \end{tikzpicture}
  \caption{Minimum spanning tree}
  \label{fig:kruskal-mst}
\end{figure}

In a real program, we sort the edges of the input graph $G$ by weight as a
pre-processing step to avoid quadratic searches for successive minima.

\begin{algorithm}
\caption{Kruskal's Algorithm}
\begin{algorithmic}
  \State $T = \emptyset$
  \State sort $E$ by edge weight \\
  \For{$(v, w) \in E$}
    \If{$(v, w)$ does not produce a cycle in $T$}
      \State add $(v, w)$ to $T$ \\
    \EndIf
  \EndFor
  \Return $T$
\end{algorithmic}
\end{algorithm}

\subsubsection{Analysis}

Sorting the edges takes $O(n\log n)$ time. Cycle detection in the inner loop
dominates the runtime of naive Kruskal's and therefore the overall runtime is
subject to the implementation details. For a brute-force cycle detection
approach, the inner loop runs $O(|E||E + V|) = O(|E||V|)$ time.

By using a Union-Find data structure, we can dramatically improve the runtime.
In particular by implementing optimizations such as Path Compression and
Union-by-Rank

> TODO: Do the detailed analysis later, (Inverse Ackermann)

\subsection{Huffman Codes}

Invented by David Huffman in the 50s as a way to compute the optimal prefix-free
variable length encoding for a (mathematical) language $\sum$. The algorithm
constructs a tree from the "bottom up", repeatedly merging the least frequently
occurring codes in order to ensure the most frequently occurring have minimum
possible depth.

\begin{algorithm}
\caption{Huffman Encoding}
\begin{algorithmic}
  \State $H = \emptyset$
  \For{symbol $\sigma \in \sum$}
    \State ${T_\sigma} = (\sigma, P_\sigma)$
    \State $H = H \cup T_\sigma$
  \EndFor
  \While{There is more than one $T_\sigma \in H$}
    \State $T_1$ = tree with minimum frequency
    \State $T_2$ = tree with 2nd smallest frequency
    \State $T_3$ = \Call{MergeTrees}{$T_1, T_2$}
    \State $H = H \cup T_3$ \\\\
  \EndWhile
\end{algorithmic}
\end{algorithm}

For example, given the frequencies:

\begin{tabular}{ |c|r| }
  \hline
  Symbol & Frequency \\
  \hline
  a & 0.60 \\
  \hline
  b & 0.25 \\
  \hline
  c & 0.10 \\
  \hline
  d & 0.05 \\
  \hline
\end{tabular}

Huffman's greedy algorithm will produce the following encoding tree

\begin{tikzpicture}[
vertex/.style={circle, draw, fill=white, thick, minimum size=20},
]

% Vertices
\node[vertex] (r1) {};
\node[vertex] (a) [below left=of r1] {$a$};
\node[vertex] (r2) [below right=of r1] {};

\node[vertex] (b) [below left=of r2] {$b$};
\node[vertex] (r3) [below right=of r2] {};

\node[vertex] (c) [below left=of r3] {$c$};
\node[vertex] (d) [below right=of r3] {$d$};

% Edges
\draw[-, thick] (r1) to node[above left] {0} (a);
\draw[-, thick] (r1) to node[above right] {1} (r2);

\draw[-, thick] (r2) to node[above left] {0} (b);
\draw[-, thick] (r2) to node[above right] {1} (r3);

\draw[-, thick] (r3) to node[above left] {0} (c);
\draw[-, thick] (r3) to node[above right] {1} (d);
\end{tikzpicture}

which ensures that symbol $a$, the most frequently encountered will be the quickest to encode and decode since its depth in minimized in the output.

\subsubsection{Analysis}

Preprocessing the nodes can be done quickly in $O(n)$ time. The inner loop of the algorithm is bound by the time it takes to select a minimum, therefore, repeated brute-force searching for minima each iteration leads to $O(n^2)$ runtime.

Once again, however, a [[Heap]] can be used to retrieve the minimum trees in constant time with $O(\log n)$ re-balance operations. This results in a much better runtime of $O(n \log n)$.

\end{document}
